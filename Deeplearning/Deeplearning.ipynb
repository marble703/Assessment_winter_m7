{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###对张量(tensor)的基本运用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 创建一个5行3列的随机张量X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9424, 0.8813, 0.8589],\n",
      "        [0.3661, 0.7537, 0.3092],\n",
      "        [0.1006, 0.4463, 0.4801],\n",
      "        [0.1152, 0.7584, 0.3923],\n",
      "        [0.6182, 0.8093, 0.0336]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(5, 3)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 用pytorch内置函数检测X的shape，dtype，device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X.dtype)\n",
    "print(X.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 直接创建一个[[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]]的张量Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [11, 12, 13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.tensor([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 用任意方法将Y的维度转变为5行3列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  6, 11],\n",
      "        [ 2,  7, 12],\n",
      "        [ 3,  8, 13],\n",
      "        [ 4,  9, 14],\n",
      "        [ 5, 10, 15]])\n"
     ]
    }
   ],
   "source": [
    "Y = Y.T\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 实现X和Y的加减乘除，同时了解abs()，sqrt()，neg()，mean()的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X+Y\n",
      "tensor([[ 1.9424,  6.8813, 11.8589],\n",
      "        [ 2.3661,  7.7537, 12.3092],\n",
      "        [ 3.1006,  8.4463, 13.4801],\n",
      "        [ 4.1152,  9.7584, 14.3923],\n",
      "        [ 5.6182, 10.8093, 15.0336]])\n",
      "X-Y\n",
      "tensor([[ -0.0576,  -5.1187, -10.1411],\n",
      "        [ -1.6339,  -6.2463, -11.6908],\n",
      "        [ -2.8994,  -7.5537, -12.5199],\n",
      "        [ -3.8848,  -8.2416, -13.6077],\n",
      "        [ -4.3818,  -9.1907, -14.9664]])\n",
      "X*Y\n",
      "tensor([[0.9424, 5.2876, 9.4478],\n",
      "        [0.7323, 5.2756, 3.7104],\n",
      "        [0.3019, 3.5708, 6.2413],\n",
      "        [0.4608, 6.8252, 5.4917],\n",
      "        [3.0912, 8.0933, 0.5036]])\n",
      "X/Y\n",
      "tensor([[0.9424, 0.1469, 0.0781],\n",
      "        [0.1831, 0.1077, 0.0258],\n",
      "        [0.0335, 0.0558, 0.0369],\n",
      "        [0.0288, 0.0843, 0.0280],\n",
      "        [0.1236, 0.0809, 0.0022]])\n"
     ]
    }
   ],
   "source": [
    "print(\"X+Y\")\n",
    "print(X+Y)\n",
    "\n",
    "print(\"X-Y\")\n",
    "print(X-Y)\n",
    "\n",
    "print(\"X*Y\")\n",
    "print(X*Y)\n",
    "\n",
    "print(\"X/Y\")\n",
    "print(X/Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abs(X-Y)\n",
      "tensor([[ 0.0576,  5.1187, 10.1411],\n",
      "        [ 1.6339,  6.2463, 11.6908],\n",
      "        [ 2.8994,  7.5537, 12.5199],\n",
      "        [ 3.8848,  8.2416, 13.6077],\n",
      "        [ 4.3818,  9.1907, 14.9664]])\n",
      "sqrt(X)\n",
      "tensor([[0.9708, 0.9388, 0.9268],\n",
      "        [0.6051, 0.8681, 0.5561],\n",
      "        [0.3172, 0.6681, 0.6929],\n",
      "        [0.3394, 0.8708, 0.6263],\n",
      "        [0.7863, 0.8996, 0.1832]])\n",
      "torch.neg(X)\n",
      "tensor([[-0.9424, -0.8813, -0.8589],\n",
      "        [-0.3661, -0.7537, -0.3092],\n",
      "        [-0.1006, -0.4463, -0.4801],\n",
      "        [-0.1152, -0.7584, -0.3923],\n",
      "        [-0.6182, -0.8093, -0.0336]])\n",
      "torch.mean(X)\n",
      "tensor(0.5244)\n"
     ]
    }
   ],
   "source": [
    "print(\"abs(X-Y)\")\n",
    "print(abs(X-Y))\n",
    "\n",
    "print(\"sqrt(X)\")\n",
    "print(torch.sqrt(X))\n",
    "\n",
    "print(\"torch.neg(X)\")#取反\n",
    "print(torch.neg(X))\n",
    "\n",
    "print(\"torch.mean(X)\")\n",
    "print(torch.mean(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. *了解内置函数max()，argmax()，sum()，同时知道其内参dim的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9424, 0.8813, 0.8589],\n",
      "        [0.3661, 0.7537, 0.3092],\n",
      "        [0.1006, 0.4463, 0.4801],\n",
      "        [0.1152, 0.7584, 0.3923],\n",
      "        [0.6182, 0.8093, 0.0336]])\n",
      "torch.max(X)\n",
      "tensor(0.9424)\n",
      "torch.argmax(X)\n",
      "tensor(0)\n",
      "torch.sum(X)\n",
      "tensor(7.8657)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(\"torch.max(X)\")\n",
    "print(torch.max(X))\n",
    "\n",
    "print(\"torch.argmax(X)\")\n",
    "print(torch.argmax(X))#最大值索引\n",
    "\n",
    "print(\"torch.sum(X)\")\n",
    "print(torch.sum(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim = 0\n",
      "tensor([0, 0, 0])\n",
      "dim = 1\n",
      "tensor([0, 1, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"dim = 0\")\n",
    "print(torch.argmax(X,dim = 0))#每列最大值索引\n",
    "\n",
    "print(\"dim = 1\")\n",
    "print(torch.argmax(X,dim = 1))#每行最大值索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 将张量X转为Numpy格式，再将其转回来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9424377  0.88127494 0.85889125]\n",
      " [0.366143   0.7536526  0.30920273]\n",
      " [0.10063541 0.44634563 0.48010284]\n",
      " [0.1152029  0.7583604  0.39226633]\n",
      " [0.6182359  0.8093324  0.03357303]]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([[0.9424, 0.8813, 0.8589],\n",
      "        [0.3661, 0.7537, 0.3092],\n",
      "        [0.1006, 0.4463, 0.4801],\n",
      "        [0.1152, 0.7584, 0.3923],\n",
      "        [0.6182, 0.8093, 0.0336]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "X_n = X.numpy()\n",
    "print(X_n)\n",
    "print(type(X_n))\n",
    "\n",
    "X_n_p = torch.from_numpy(X_n)\n",
    "print(X_n_p)\n",
    "print(type(X_n_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  8. 将张量X放到cuda上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9424, 0.8813, 0.8589],\n",
       "        [0.3661, 0.7537, 0.3092],\n",
       "        [0.1006, 0.4463, 0.4801],\n",
       "        [0.1152, 0.7584, 0.3923],\n",
       "        [0.6182, 0.8093, 0.0336]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "X.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  9. *学会张量的拼接，解压，压缩，广播，以及Numpy的transpose函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 拼接:\n",
    "两个矩阵有一个维度相等时，将 Tensor 拼接，  \n",
    "dim = 0 拼接行  \n",
    "dim = 1 拼接列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n",
      "tensor([[ 0.9424,  0.8813,  0.8589],\n",
      "        [ 0.3661,  0.7537,  0.3092],\n",
      "        [ 0.1006,  0.4463,  0.4801],\n",
      "        [ 0.1152,  0.7584,  0.3923],\n",
      "        [ 0.6182,  0.8093,  0.0336],\n",
      "        [ 1.0000,  6.0000, 11.0000],\n",
      "        [ 2.0000,  7.0000, 12.0000],\n",
      "        [ 3.0000,  8.0000, 13.0000],\n",
      "        [ 4.0000,  9.0000, 14.0000],\n",
      "        [ 5.0000, 10.0000, 15.0000]])\n",
      "torch.Size([10, 3])\n",
      "tensor([[ 0.9424,  0.8813,  0.8589,  1.0000,  6.0000, 11.0000],\n",
      "        [ 0.3661,  0.7537,  0.3092,  2.0000,  7.0000, 12.0000],\n",
      "        [ 0.1006,  0.4463,  0.4801,  3.0000,  8.0000, 13.0000],\n",
      "        [ 0.1152,  0.7584,  0.3923,  4.0000,  9.0000, 14.0000],\n",
      "        [ 0.6182,  0.8093,  0.0336,  5.0000, 10.0000, 15.0000]])\n",
      "torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "X_Y=torch.cat((X,Y),0)\n",
    "print(X_Y)\n",
    "print(X_Y.shape)\n",
    "\n",
    "Y_X=torch.cat((X,Y),1)\n",
    "print(Y_X)\n",
    "print(Y_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 广播:  \n",
    "对两个形状不同的 Tensor 运算时，复制元素使这两个 Tensor 形状相同,再运算。  \n",
    "每个维度的大小要么相等，要么其中一个为 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "tensor([1, 2, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(x \u001b[38;5;241m+\u001b[39m y)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
